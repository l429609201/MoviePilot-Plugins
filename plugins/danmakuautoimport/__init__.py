import time
import uuid
from datetime import datetime
from pathlib import Path
from threading import Lock
from typing import Any, Dict, List, Optional, Tuple

import pytz
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

from app.core.config import settings
from app.core.event import Event, eventmanager
from app.log import logger
from app.plugins import _PluginBase
from app.schemas import MediaType, NotificationType
from app.schemas.types import EventType
from app.utils.http import RequestUtils


class DanmakuAutoImport(_PluginBase):
    # æ’ä»¶åç§°
    plugin_name = "å¼¹å¹•åº“è‡ªåŠ¨å¯¼å…¥"
    # æ’ä»¶æè¿°
    plugin_desc = "åª’ä½“ä¸‹è½½å®Œæˆåè‡ªåŠ¨æ¨é€è‡³å¼¹å¹•åº“ä¸‹è½½å¼¹å¹•,æ”¯æŒä»»åŠ¡é˜Ÿåˆ—å’Œå®šæ—¶å¤„ç†"
    # æ’ä»¶å›¾æ ‡
    plugin_icon = "https://raw.githubusercontent.com/l429609201/MoviePilot-Plugins/refs/heads/main/icons/danmaku.png"
    # æ’ä»¶ç‰ˆæœ¬
    plugin_version = "1.0.0"
    # æ’ä»¶ä½œè€…
    plugin_author = "Misaka10876"
    # ä½œè€…ä¸»é¡µ
    author_url = "https://github.com/l429609201"
    # æ’ä»¶é…ç½®é¡¹IDå‰ç¼€
    plugin_config_prefix = "danmakuautoimport_"
    # åŠ è½½é¡ºåº
    plugin_order = 20
    # å¯ä½¿ç”¨çš„ç”¨æˆ·çº§åˆ«
    auth_level = 1

    # ç§æœ‰å±æ€§
    _enabled = False
    _notify = False
    _danmu_server_url = ""
    _external_api_key = ""
    _cron = "*/5 * * * *"
    _delay_seconds = 0
    _max_queue_size = 100
    _process_batch_size = 1
    _only_anime = False
    _search_type = "tmdb"
    _task_progress = {}  # ä»»åŠ¡è¿›åº¦å­—å…¸ {task_id: progress}
    _auto_retry = True
    _retry_count = 3

    # ä»»åŠ¡é˜Ÿåˆ—
    _buffer_tasks: List[Dict[str, Any]] = []  # ç¼“å†²åŒº
    _pending_tasks: List[Dict[str, Any]] = []
    _processing_tasks: Dict[str, Dict[str, Any]] = {}
    _lock = Lock()
    _consolidate_interval = 30  # æ•´åˆé—´éš”(ç§’)
    _consolidate_countdown = 30  # æ•´åˆå€’è®¡æ—¶(ç§’)

    def init_plugin(self, config: dict = None):
        """åˆå§‹åŒ–æ’ä»¶"""
        if config:
            self._enabled = config.get("enable", False)  # âœ… ä¿®å¤å­—æ®µå: enabled -> enable
            self._notify = config.get("notify", False)
            self._danmu_server_url = config.get("danmu_server_url", "").rstrip("/")
            self._external_api_key = config.get("external_api_key", "")
            self._cron = config.get("cron", "*/5 * * * *")
            # æ”¯æŒdelay_hours(å°æ—¶)å’Œdelay_seconds(ç§’),ä¼˜å…ˆä½¿ç”¨delay_hours
            delay_hours = config.get("delay_hours")
            if delay_hours is not None:
                self._delay_seconds = int(delay_hours) * 3600
            else:
                self._delay_seconds = int(config.get("delay_seconds", 0))
            self._max_queue_size = int(config.get("max_queue_size", 100))
            self._process_batch_size = int(config.get("process_batch_size", 1))
            self._only_anime = config.get("only_anime", False)
            self._search_type = config.get("search_type", "tmdb")
            self._auto_retry = config.get("auto_retry", True)
            self._retry_count = int(config.get("retry_count", 3))

        # åˆå§‹åŒ–é˜Ÿåˆ—
        self._buffer_tasks = []
        self._pending_tasks = []
        self._processing_tasks = {}
        self._last_consolidate_time = datetime.now(tz=pytz.timezone(settings.TZ))

    def get_state(self) -> bool:
        """è·å–æ’ä»¶çŠ¶æ€"""
        return self._enabled and bool(self._danmu_server_url) and bool(self._external_api_key)

    @staticmethod
    def get_command() -> List[Dict[str, Any]]:
        """å®šä¹‰è¿œç¨‹æ§åˆ¶å‘½ä»¤"""
        return [
            {
                "cmd": "/danmaku_queue",
                "event": EventType.PluginAction,
                "desc": "æŸ¥çœ‹å¼¹å¹•å¯¼å…¥é˜Ÿåˆ—",
                "category": "å¼¹å¹•",
                "data": {"action": "view_queue"}
            },
            {
                "cmd": "/danmaku_clear",
                "event": EventType.PluginAction,
                "desc": "æ¸…ç©ºå¼¹å¹•å¯¼å…¥é˜Ÿåˆ—",
                "category": "å¼¹å¹•",
                "data": {"action": "clear_queue"}
            }
        ]

    def get_service(self) -> List[Dict[str, Any]]:
        """æ³¨å†Œå®šæ—¶æœåŠ¡"""
        if not self._enabled:
            return []

        services = []

        # é˜Ÿåˆ—å¤„ç†å®šæ—¶ä»»åŠ¡
        if self._cron:
            services.append({
                "id": "DanmakuAutoImport",
                "name": "å¼¹å¹•è‡ªåŠ¨å¯¼å…¥å®šæ—¶ä»»åŠ¡",
                "trigger": CronTrigger.from_crontab(self._cron),
                "func": self._process_queue,
                "kwargs": {}
            })

        # æ•´åˆå®šæ—¶ä»»åŠ¡ - æ¯1ç§’æ‰§è¡Œä¸€æ¬¡å€’è®¡æ—¶
        services.append({
            "id": "DanmakuAutoImportConsolidate",
            "name": "å¼¹å¹•è‡ªåŠ¨å¯¼å…¥æ•´åˆä»»åŠ¡",
            "trigger": IntervalTrigger(seconds=1),
            "func": self._consolidate_tick,
            "kwargs": {}
        })

        # æ¸…ç†æˆåŠŸä»»åŠ¡å®šæ—¶ä»»åŠ¡ - æ¯å¤©0ç‚¹å’Œ12ç‚¹æ‰§è¡Œ
        services.append({
            "id": "DanmakuAutoImportCleanup",
            "name": "å¼¹å¹•è‡ªåŠ¨å¯¼å…¥æ¸…ç†æˆåŠŸä»»åŠ¡",
            "trigger": CronTrigger.from_crontab("0 0,12 * * *"),
            "func": self._cleanup_success_tasks,
            "kwargs": {}
        })

        return services

    @eventmanager.register(EventType.TransferComplete)
    def on_transfer_complete(self, event: Event):
        """ç›‘å¬åª’ä½“è½¬ç§»å®Œæˆäº‹ä»¶"""
        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ========== on_transfer_completeè¢«è°ƒç”¨ ==========")
        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: _enabled={self._enabled}, _danmu_server_url={'å·²é…ç½®' if self._danmu_server_url else 'æœªé…ç½®'}, "
                   f"_external_api_key={'å·²é…ç½®' if self._external_api_key else 'æœªé…ç½®'}")

        if not self._enabled:
            logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ’ä»¶æœªå¯ç”¨,è·³è¿‡å¤„ç†")
            return

        # è°ƒè¯•: æ‰“å°äº‹ä»¶å¯¹è±¡ä¿¡æ¯
        logger.debug(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°TransferCompleteäº‹ä»¶, eventç±»å‹={type(event)}")

        event_data = event.event_data or {}
        logger.debug(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: event_dataé”®åˆ—è¡¨={list(event_data.keys())}")

        mediainfo = event_data.get("mediainfo")
        meta = event_data.get("meta")

        if not mediainfo:
            logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æœªè·å–åˆ°åª’ä½“ä¿¡æ¯, event_dataå†…å®¹={event_data}")
            return

        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°è½¬ç§»å®Œæˆäº‹ä»¶ - {mediainfo.title}, ç±»å‹={mediainfo.type}")

        # å¦‚æœåªå¤„ç†åŠ¨æ¼«ä¸”å½“å‰ä¸æ˜¯åŠ¨æ¼«ï¼Œåˆ™è·³è¿‡
        if self._only_anime and mediainfo.type != MediaType.TV:
            logger.debug(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·³è¿‡éåŠ¨æ¼«åª’ä½“ {mediainfo.title}")
            return

        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self._add_to_buffer(mediainfo, meta)

    @eventmanager.register(EventType.PluginAction)
    def on_plugin_action(self, event: Event):
        """å¤„ç†æ’ä»¶åŠ¨ä½œäº‹ä»¶"""
        if not event:
            return

        event_data = event.event_data or {}
        if not event_data:
            return

        action = event_data.get("action")
        if action == "view_queue":
            self._view_queue(event_data)
        elif action == "clear_queue":
            self._clear_queue(event_data)

    def _add_to_buffer(self, mediainfo, meta):
        """æ·»åŠ ä»»åŠ¡åˆ°ç¼“å†²åŒº"""
        with self._lock:
            # åˆ›å»ºç¼“å†²ä»»åŠ¡
            task = {
                "id": str(uuid.uuid4()),
                "mediainfo": mediainfo,
                "meta": meta,
                "add_time": datetime.now(tz=pytz.timezone(settings.TZ))
            }

            self._buffer_tasks.append(task)
            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²æ·»åŠ åˆ°ç¼“å†²åŒº - {mediainfo.title} (ç¼“å†²åŒºé•¿åº¦: {len(self._buffer_tasks)})")

    def _consolidate_tick(self):
        """æ•´åˆå®šæ—¶å™¨tick - æ¯ç§’æ‰§è¡Œä¸€æ¬¡"""
        should_consolidate = False

        with self._lock:
            # å€’è®¡æ—¶é€’å‡
            if self._consolidate_countdown > 0:
                self._consolidate_countdown -= 1

            # å€’è®¡æ—¶ç»“æŸ,æ ‡è®°éœ€è¦æ•´åˆ
            if self._consolidate_countdown == 0:
                self._consolidate_countdown = self._consolidate_interval
                should_consolidate = True

        # åœ¨é”å¤–è°ƒç”¨æ•´åˆ(é¿å…æ­»é”)
        if should_consolidate:
            self._consolidate_buffer(force=False)

    def _consolidate_buffer(self, force: bool = False):
        """æ•´åˆç¼“å†²åŒºä»»åŠ¡

        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ•´åˆ(å¿½ç•¥æ—¶é—´é—´éš”)
        """
        with self._lock:
            # å¦‚æœç¼“å†²åŒºä¸ºç©º,ç›´æ¥è¿”å›
            if not self._buffer_tasks:
                # å¦‚æœæ˜¯å¼ºåˆ¶æ•´åˆ,ä»ç„¶é‡ç½®å€’è®¡æ—¶
                if force:
                    self._consolidate_countdown = self._consolidate_interval
                return

            # å¦‚æœæ˜¯å¼ºåˆ¶æ•´åˆ,é‡ç½®å€’è®¡æ—¶
            if force:
                self._consolidate_countdown = self._consolidate_interval

            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¼€å§‹æ•´åˆç¼“å†²åŒºä»»åŠ¡ (ç¼“å†²åŒºé•¿åº¦: {len(self._buffer_tasks)}, å¼ºåˆ¶: {force})")

            # æŒ‰tmdb_idå’Œmedia_typeåˆ†ç»„
            groups = {}
            for task in self._buffer_tasks:
                mediainfo = task["mediainfo"]
                meta = task["meta"]

                # ç”µå½±ç›´æ¥æ·»åŠ ,ä¸æ•´åˆ
                if mediainfo.type == MediaType.MOVIE:
                    self._add_to_queue_direct(task)
                    continue

                # ç”µè§†å‰§æŒ‰tmdb_idåˆ†ç»„
                key = f"{mediainfo.tmdb_id}_{mediainfo.type.value}"
                if key not in groups:
                    groups[key] = {
                        "mediainfo": mediainfo,
                        "episodes": []
                    }

                # æ·»åŠ é›†æ•°ä¿¡æ¯
                episode_info = {}
                if meta:
                    if hasattr(meta, "begin_season") and meta.begin_season:
                        episode_info["season"] = meta.begin_season
                    if hasattr(meta, "begin_episode") and meta.begin_episode:
                        episode_info["episode"] = meta.begin_episode

                if episode_info:
                    groups[key]["episodes"].append(episode_info)

            # åˆ›å»ºæ•´åˆä»»åŠ¡
            for key, group in groups.items():
                task = {
                    "id": str(uuid.uuid4()),
                    "mediainfo": group["mediainfo"],
                    "meta": None,
                    "episodes": group["episodes"],  # é›†æ•°åˆ—è¡¨
                    "add_time": datetime.now(tz=pytz.timezone(settings.TZ)),
                    "retry_count": 0,
                    "status": "pending",
                    "error_msg": None,
                    "danmu_task_id": None,
                    "is_consolidated": True  # æ ‡è®°ä¸ºæ•´åˆä»»åŠ¡
                }

                # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
                if len(self._pending_tasks) >= self._max_queue_size:
                    logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: é˜Ÿåˆ—å·²æ»¡({self._max_queue_size}),åœæ­¢æ•´åˆ")
                    break

                self._pending_tasks.append(task)
                logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²æ•´åˆä»»åŠ¡ - {group['mediainfo'].title} ({len(group['episodes'])}é›†)")

            # æ¸…ç©ºç¼“å†²åŒº
            self._buffer_tasks.clear()
            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ç¼“å†²åŒºæ•´åˆå®Œæˆ (å¾…å¤„ç†é˜Ÿåˆ—é•¿åº¦: {len(self._pending_tasks)})")

    def _add_to_queue_direct(self, buffer_task):
        """ç›´æ¥æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—(ä¸æ•´åˆ)"""
        mediainfo = buffer_task["mediainfo"]
        meta = buffer_task["meta"]

        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
        if len(self._pending_tasks) >= self._max_queue_size:
            logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: é˜Ÿåˆ—å·²æ»¡({self._max_queue_size}),è·³è¿‡æ·»åŠ ")
            return

        # åˆ›å»ºä»»åŠ¡
        task = {
            "id": str(uuid.uuid4()),
            "mediainfo": mediainfo,
            "meta": meta,
            "episodes": None,  # ç”µå½±æ— é›†æ•°
            "add_time": datetime.now(tz=pytz.timezone(settings.TZ)),
            "retry_count": 0,
            "status": "pending",
            "error_msg": None,
            "danmu_task_id": None,
            "is_consolidated": False
        }

        self._pending_tasks.append(task)
        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ— - {mediainfo.title}")

    def _process_queue(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
        if not self._enabled:
            return

        # å…ˆæ•´åˆç¼“å†²åŒº
        self._consolidate_buffer()

        logger.info("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¼€å§‹å¤„ç†é˜Ÿåˆ—ä»»åŠ¡")

        with self._lock:
            # è·å–å¾…å¤„ç†ä»»åŠ¡
            tasks_to_process = self._pending_tasks[:self._process_batch_size]
            if not tasks_to_process:
                logger.debug("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: é˜Ÿåˆ—ä¸ºç©º,æ— éœ€å¤„ç†")
                return

            # ä»å¾…å¤„ç†é˜Ÿåˆ—ç§»é™¤
            for task in tasks_to_process:
                self._pending_tasks.remove(task)
                self._processing_tasks[task["id"]] = task

        # å¤„ç†æ¯ä¸ªä»»åŠ¡
        for task in tasks_to_process:
            # æ£€æŸ¥å»¶æ—¶
            if self._delay_seconds > 0:
                add_time = task["add_time"]
                now = datetime.now(tz=pytz.timezone(settings.TZ))
                elapsed = (now - add_time).total_seconds()
                if elapsed < self._delay_seconds:
                    logger.debug(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ä»»åŠ¡ {task['id']} å»¶æ—¶æœªåˆ°,è·³è¿‡å¤„ç†")
                    with self._lock:
                        self._pending_tasks.append(task)
                        del self._processing_tasks[task["id"]]
                    continue

            # å¯¼å…¥å¼¹å¹•
            self._import_danmaku(task)

    def _import_danmaku(self, task: dict):
        """å¯¼å…¥å¼¹å¹•"""
        task_id = task["id"]
        mediainfo = task["mediainfo"]
        meta = task["meta"]

        try:
            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¼€å§‹å¯¼å…¥ {mediainfo.title}")

            # åˆå§‹åŒ–è¿›åº¦
            self._task_progress[task_id] = 0

            # æ„å»ºAPIè¯·æ±‚
            api_url = f"{self._danmu_server_url}/api/control/import/auto"

            # æ›´æ–°è¿›åº¦: å‡†å¤‡è¯·æ±‚
            self._task_progress[task_id] = 20

            # æ„å»ºè¯·æ±‚å‚æ•°(å¤–éƒ¨APIä½¿ç”¨Queryå‚æ•°,ä¸æ˜¯JSON body)
            params = {
                "api_key": self._external_api_key,
                "searchType": self._search_type,
                "searchTerm": str(mediainfo.tmdb_id) if self._search_type == "tmdb" else mediainfo.title,
                "mediaType": "tv_series" if mediainfo.type == MediaType.TV else "movie"
            }

            # å¦‚æœæ˜¯å‰§é›†,æ·»åŠ å­£é›†ä¿¡æ¯
            if mediainfo.type == MediaType.TV and meta:
                if hasattr(meta, "begin_season") and meta.begin_season:
                    params["season"] = meta.begin_season
                if hasattr(meta, "begin_episode") and meta.begin_episode:
                    params["episode"] = meta.begin_episode

            # æ›´æ–°è¿›åº¦: å‘é€è¯·æ±‚
            self._task_progress[task_id] = 40

            # å‘é€POSTè¯·æ±‚(å‚æ•°åœ¨URLä¸­)
            response = RequestUtils(timeout=30).post_res(url=api_url, params=params)
            if not response or response.status_code != 202:
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status_code if response else 'No response'}")

            # æ›´æ–°è¿›åº¦: å¤„ç†å“åº”
            self._task_progress[task_id] = 70

            result = response.json()
            danmu_task_id = result.get("taskId")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task["status"] = "success"
            task["danmu_task_id"] = danmu_task_id

            # æ›´æ–°è¿›åº¦: å®Œæˆ
            self._task_progress[task_id] = 100

            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¯¼å…¥æˆåŠŸ {mediainfo.title} - ä»»åŠ¡ID: {danmu_task_id}")

            # å‘é€é€šçŸ¥
            if self._notify:
                self.post_message(
                    mtype=NotificationType.SiteMessage,
                    title="å¼¹å¹•å¯¼å…¥æˆåŠŸ",
                    text=f"å·²æˆåŠŸå¯¼å…¥ {mediainfo.title} çš„å¼¹å¹•\nä»»åŠ¡ID: {danmu_task_id}"
                )

        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¯¼å…¥å¤±è´¥ {mediainfo.title} - {str(e)}")
            task["error_msg"] = str(e)
            task["retry_count"] += 1

            # æ ‡è®°è¿›åº¦ä¸ºå¤±è´¥
            self._task_progress[task_id] = -1

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
            if self._auto_retry and task["retry_count"] < self._retry_count:
                logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ä»»åŠ¡ {task_id} å°†é‡è¯• (ç¬¬{task['retry_count']}æ¬¡)")
                with self._lock:
                    self._pending_tasks.append(task)
            else:
                task["status"] = "failed"
                logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ä»»åŠ¡ {task_id} å¤±è´¥,å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")

        finally:
            # ä»å¤„ç†é˜Ÿåˆ—ç§»é™¤
            with self._lock:
                if task_id in self._processing_tasks:
                    del self._processing_tasks[task_id]
                # æ¸…ç†è¿›åº¦æ•°æ®(å»¶è¿Ÿ5ç§’,è®©å‰ç«¯æœ‰æ—¶é—´æ˜¾ç¤ºå®ŒæˆçŠ¶æ€)
                if task_id in self._task_progress:
                    time.sleep(5)
                    del self._task_progress[task_id]

    def _view_queue(self, event_data: dict):
        """æŸ¥çœ‹é˜Ÿåˆ—"""
        with self._lock:
            pending_count = len(self._pending_tasks)
            processing_count = len(self._processing_tasks)

        message = f"ğŸ“Š å¼¹å¹•å¯¼å…¥é˜Ÿåˆ—çŠ¶æ€\n\n"
        message += f"â³ å¾…å¤„ç†: {pending_count} ä¸ªä»»åŠ¡\n"
        message += f"ğŸ”„ å¤„ç†ä¸­: {processing_count} ä¸ªä»»åŠ¡\n"
        message += f"ğŸ“¦ é˜Ÿåˆ—å®¹é‡: {self._max_queue_size}\n"

        self.post_message(
            mtype=NotificationType.SiteMessage,
            title="å¼¹å¹•å¯¼å…¥é˜Ÿåˆ—",
            text=message
        )

    def _clear_queue(self, event_data: dict):
        """æ¸…ç©ºé˜Ÿåˆ—"""
        with self._lock:
            cleared_count = len(self._pending_tasks)
            self._pending_tasks = []

        message = f"ğŸ—‘ï¸ å·²æ¸…ç©ºå¼¹å¹•å¯¼å…¥é˜Ÿåˆ—\n\næ¸…é™¤äº† {cleared_count} ä¸ªå¾…å¤„ç†ä»»åŠ¡"

        self.post_message(
            mtype=NotificationType.SiteMessage,
            title="æ¸…ç©ºé˜Ÿåˆ—",
            text=message
        )

        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²æ¸…ç©ºé˜Ÿåˆ—,æ¸…é™¤äº† {cleared_count} ä¸ªä»»åŠ¡")

    def _cleanup_success_tasks(self):
        """æ¸…ç†çŠ¶æ€ä¸ºæˆåŠŸçš„ä»»åŠ¡"""
        with self._lock:
            # è¿‡æ»¤å‡ºéæˆåŠŸçŠ¶æ€çš„ä»»åŠ¡
            before_count = len(self._pending_tasks)
            self._pending_tasks = [task for task in self._pending_tasks if task.get('status') != 'success']
            after_count = len(self._pending_tasks)
            cleaned_count = before_count - after_count

        if cleaned_count > 0:
            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å®šæ—¶æ¸…ç†æˆåŠŸä»»åŠ¡,å…±æ¸…ç† {cleaned_count} ä¸ªä»»åŠ¡")

    # ========== V2 API æ¥å£ ==========

    def _trigger_manual_process(self) -> Dict[str, Any]:
        """API: æ‰‹åŠ¨è§¦å‘å¤„ç†"""
        logger.info("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°æ‰‹åŠ¨å¤„ç†è¯·æ±‚")
        if not self._enabled:
            return {"message": "æ’ä»¶å·²ç¦ç”¨,æ— æ³•æ‰§è¡Œå¤„ç†", "error": True}

        try:
            self._process_queue()
            return {"message": "å¤„ç†ä»»åŠ¡å·²å®Œæˆ"}
        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ‰‹åŠ¨å¤„ç†å¤±è´¥ - {e}")
            return {"message": f"æ‰‹åŠ¨å¤„ç†å¤±è´¥: {e}", "error": True}

    def _trigger_manual_consolidate(self) -> Dict[str, Any]:
        """API: æ‰‹åŠ¨è§¦å‘æ•´åˆ"""
        logger.info("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°æ‰‹åŠ¨æ•´åˆè¯·æ±‚")
        if not self._enabled:
            return {"success": False, "message": "æ’ä»¶å·²ç¦ç”¨,æ— æ³•æ‰§è¡Œæ•´åˆ"}

        try:
            with self._lock:
                buffer_count = len(self._buffer_tasks)
                if buffer_count == 0:
                    return {"success": False, "message": "ç¼“å†²åŒºä¸ºç©º,æ— éœ€æ•´åˆ"}

            # å¼ºåˆ¶æ•´åˆ(å¿½ç•¥æ—¶é—´é—´éš”)
            self._consolidate_buffer(force=True)

            with self._lock:
                pending_count = len(self._pending_tasks)

            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ‰‹åŠ¨æ•´åˆå®Œæˆ,å¾…å¤„ç†é˜Ÿåˆ—é•¿åº¦: {pending_count}")
            return {"success": True, "message": f"æ•´åˆå®Œæˆ,å·²æ·»åŠ  {pending_count} ä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ—"}
        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ‰‹åŠ¨æ•´åˆå¤±è´¥ - {e}")
            return {"success": False, "message": f"æ‰‹åŠ¨æ•´åˆå¤±è´¥: {e}"}

    def _trigger_import_task(self, payload: dict = None) -> Dict[str, Any]:
        """API: æ‰‹åŠ¨è§¦å‘å•æ¡ä»»åŠ¡å¯¼å…¥"""
        if payload is None:
            logger.error("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¯¼å…¥ä»»åŠ¡è¯·æ±‚æ•°æ®ä¸ºç©º")
            return {"success": False, "message": "è¯·æ±‚æ•°æ®ä¸ºç©º"}

        task_id = payload.get('task_id')
        if not task_id:
            return {"success": False, "message": "ç¼ºå°‘task_idå‚æ•°"}

        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°æ‰‹åŠ¨å¯¼å…¥è¯·æ±‚ - task_id={task_id}")

        if not self._enabled:
            return {"success": False, "message": "æ’ä»¶å·²ç¦ç”¨,æ— æ³•æ‰§è¡Œå¯¼å…¥"}

        try:
            # æŸ¥æ‰¾ä»»åŠ¡
            task = None
            with self._lock:
                for t in self._pending_tasks:
                    if t.get('id') == task_id:  # ä½¿ç”¨'id'è€Œä¸æ˜¯'task_id'
                        task = t
                        break

                if not task:
                    return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if task.get('status') == 'processing':
                    return {"success": False, "message": "ä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­"}

                # æ ‡è®°ä¸ºå¤„ç†ä¸­
                task['status'] = 'processing'
                self._processing_tasks[task_id] = task

            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¯¼å…¥
            import threading
            thread = threading.Thread(target=self._import_danmaku, args=(task,))
            thread.daemon = True
            thread.start()

            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²å¯åŠ¨å¯¼å…¥ä»»åŠ¡ - {task_id}")
            return {"success": True, "message": "å¯¼å…¥ä»»åŠ¡å·²å¯åŠ¨"}
        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ‰‹åŠ¨å¯¼å…¥å¤±è´¥ - {e}")
            return {"success": False, "message": f"æ‰‹åŠ¨å¯¼å…¥å¤±è´¥: {e}"}

    def _get_status(self) -> Dict[str, Any]:
        """API: è·å–çŠ¶æ€"""
        with self._lock:
            pending_tasks = [
                {
                    "id": task["id"],
                    "title": task["mediainfo"].title if task.get("mediainfo") else "æœªçŸ¥",
                    "add_time": task["add_time"].strftime("%Y-%m-%d %H:%M:%S"),
                    "retry_count": task["retry_count"],
                    "status": task["status"]
                }
                for task in self._pending_tasks
            ]

            processing_tasks = [
                {
                    "id": task["id"],
                    "title": task["mediainfo"].title if task.get("mediainfo") else "æœªçŸ¥",
                    "add_time": task["add_time"].strftime("%Y-%m-%d %H:%M:%S"),
                    "retry_count": task["retry_count"],
                    "status": task["status"]
                }
                for task in self._processing_tasks.values()
            ]

        return {
            "enabled": self._enabled,
            "cron": self._cron,
            "pending_count": len(pending_tasks),
            "processing_count": len(processing_tasks),
            "pending_tasks": pending_tasks,
            "processing_tasks": processing_tasks,
            "max_queue_size": self._max_queue_size
        }

    def _clear_queue_api(self) -> Dict[str, Any]:
        """API: æ¸…ç©ºé˜Ÿåˆ—"""
        with self._lock:
            cleared_count = len(self._pending_tasks)
            self._pending_tasks = []

        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å·²æ¸…ç©ºé˜Ÿåˆ—,æ¸…é™¤äº† {cleared_count} ä¸ªä»»åŠ¡")
        return {"message": f"å·²æ¸…ç©ºé˜Ÿåˆ—,æ¸…é™¤äº† {cleared_count} ä¸ªä»»åŠ¡", "cleared_count": cleared_count}

    @staticmethod
    def get_render_mode() -> Tuple[str, Optional[str]]:
        """å£°æ˜Vueæ¸²æŸ“æ¨¡å¼å’Œé™æ€èµ„æºè·¯å¾„"""
        return "vue", "dist/assets"

    def _get_config(self) -> Dict[str, Any]:
        """APIç«¯ç‚¹: è¿”å›å½“å‰æ’ä»¶é…ç½®"""
        return {
            "enable": self._enabled,
            "notify": self._notify,
            "danmu_server_url": self._danmu_server_url,
            "external_api_key": self._external_api_key,
            "cron": self._cron,
            "delay_hours": self._delay_seconds // 3600,  # è½¬æ¢ä¸ºå°æ—¶
            "max_queue_size": self._max_queue_size,
            "process_batch_size": self._process_batch_size,
            "only_anime": self._only_anime,
            "search_type": self._search_type,
            "auto_retry": self._auto_retry,
            "retry_count": self._retry_count
        }

    def _save_config(self, config_payload: dict = None) -> Dict[str, Any]:
        """APIç«¯ç‚¹: ä¿å­˜æ’ä»¶é…ç½®"""
        logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ”¶åˆ°é…ç½®ä¿å­˜è¯·æ±‚: {config_payload}")

        # é˜²å¾¡æ€§æ£€æŸ¥
        if config_payload is None:
            logger.error("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: é…ç½®æ•°æ®ä¸ºç©º")
            return {"success": False, "message": "é…ç½®æ•°æ®ä¸ºç©º", "saved_config": self._get_config()}

        try:
            # æ›´æ–°å®ä¾‹å˜é‡
            self._enabled = config_payload.get('enable', self._enabled)
            self._notify = config_payload.get('notify', self._notify)
            self._danmu_server_url = config_payload.get('danmu_server_url', self._danmu_server_url)
            self._external_api_key = config_payload.get('external_api_key', self._external_api_key)
            self._cron = config_payload.get('cron', self._cron)
            # æ”¯æŒdelay_hours(å°æ—¶)å’Œdelay_seconds(ç§’),ä¼˜å…ˆä½¿ç”¨delay_hours
            delay_hours = config_payload.get('delay_hours')
            if delay_hours is not None:
                self._delay_seconds = int(delay_hours) * 3600
            else:
                self._delay_seconds = int(config_payload.get('delay_seconds', self._delay_seconds))
            self._max_queue_size = int(config_payload.get('max_queue_size', self._max_queue_size))
            self._process_batch_size = int(config_payload.get('process_batch_size', self._process_batch_size))
            self._only_anime = config_payload.get('only_anime', self._only_anime)
            self._search_type = config_payload.get('search_type', self._search_type)
            self._auto_retry = config_payload.get('auto_retry', self._auto_retry)
            self._retry_count = int(config_payload.get('retry_count', self._retry_count))

            # å‡†å¤‡ä¿å­˜çš„é…ç½®
            config_to_save = {
                "enable": self._enabled,
                "notify": self._notify,
                "danmu_server_url": self._danmu_server_url,
                "external_api_key": self._external_api_key,
                "cron": self._cron,
                "delay_hours": self._delay_seconds // 3600,  # è½¬æ¢ä¸ºå°æ—¶ä¿å­˜
                "max_queue_size": self._max_queue_size,
                "process_batch_size": self._process_batch_size,
                "only_anime": self._only_anime,
                "search_type": self._search_type,
                "auto_retry": self._auto_retry,
                "retry_count": self._retry_count
            }

            # ä¿å­˜é…ç½®
            self.update_config(config_to_save)

            # é‡æ–°åˆå§‹åŒ–æ’ä»¶ - ä»æ•°æ®åº“è¯»å–é…ç½®(å‚è€ƒå®˜æ–¹æ’ä»¶logsclean)
            self.stop_service()
            self.init_plugin(self.get_config())

            # æ‰‹åŠ¨æ›´æ–°äº‹ä»¶å¤„ç†å™¨çŠ¶æ€ - æ ¹æ®enableå¼€å…³æ§åˆ¶
            try:
                from app.core.event import eventmanager
                if self.get_state():
                    eventmanager.enable_event_handler(type(self))
                    logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: äº‹ä»¶å¤„ç†å™¨å·²å¯ç”¨")
                else:
                    eventmanager.disable_event_handler(type(self))
                    logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: äº‹ä»¶å¤„ç†å™¨å·²ç¦ç”¨")
            except Exception as e:
                logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: æ›´æ–°äº‹ä»¶å¤„ç†å™¨çŠ¶æ€å¤±è´¥: {e}")

            logger.info(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: é…ç½®å·²ä¿å­˜å¹¶é‡æ–°åˆå§‹åŒ–")

            return {"success": True, "message": "é…ç½®å·²æˆåŠŸä¿å­˜", "saved_config": self._get_config()}

        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: ä¿å­˜é…ç½®å¤±è´¥: {e}", exc_info=True)
            return {"success": False, "message": f"ä¿å­˜é…ç½®å¤±è´¥: {e}", "saved_config": self._get_config()}

    def _get_queue_stats(self) -> Dict[str, Any]:
        """APIç«¯ç‚¹: è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            # è·å–ä¸‹æ¬¡è¿è¡Œæ—¶é—´
            next_run_time = 'N/A'
            if hasattr(self, '_scheduler') and self._scheduler:
                jobs = self._scheduler.get_jobs()
                if jobs:
                    next_run = jobs[0].next_run_time
                    if next_run:
                        next_run_time = next_run.strftime('%Y-%m-%d %H:%M:%S')

            # è·å–æœ€è¿‘å¤„ç†å†å²(æœ€å¤š5æ¡)
            last_run_results = []
            # è¿™é‡Œå¯ä»¥ä»self._processing_tasksæˆ–å†å²è®°å½•ä¸­è·å–
            # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨,åç»­å¯ä»¥æ·»åŠ å†å²è®°å½•åŠŸèƒ½

            return {
                "enabled": self._enabled,
                "pending": len(self._pending_tasks),
                "processing": len(self._processing_tasks),
                "max_queue_size": self._max_queue_size,
                "cron": self._cron,
                "next_run_time": next_run_time,
                "last_run_results": last_run_results
            }

    def _get_pending_tasks(self):
        """APIç«¯ç‚¹: è·å–å¾…å¤„ç†ä»»åŠ¡åˆ—è¡¨å’Œç¼“å†²åŒºçŠ¶æ€"""
        try:
            # ç¡®ä¿_pending_taskså·²åˆå§‹åŒ–
            if not hasattr(self, '_pending_tasks'):
                logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: _pending_tasksæœªåˆå§‹åŒ–,è¿”å›ç©ºæ•°æ®")
                return {
                    "buffer_count": 0,
                    "consolidate_countdown": 0,
                    "tasks": []
                }

            with self._lock:
                # è¿”å›å½“å‰å€’è®¡æ—¶å€¼
                consolidate_countdown = max(0, self._consolidate_countdown)

                result = []

                for task in self._pending_tasks[:50]:  # æœ€å¤šè¿”å›50ä¸ª
                    try:
                        mediainfo = task.get('mediainfo')
                        if not mediainfo:
                            logger.debug(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·³è¿‡æ— mediainfoçš„ä»»åŠ¡")
                            continue

                        # å®‰å…¨è·å–add_time
                        add_time_str = 'æœªçŸ¥'
                        add_time = task.get('add_time')
                        if add_time and hasattr(add_time, 'strftime'):
                            try:
                                add_time_str = add_time.strftime('%Y-%m-%d %H:%M:%S')
                            except Exception:
                                add_time_str = str(add_time)

                        # å®‰å…¨è·å–media_type
                        media_type_str = 'æœªçŸ¥'
                        try:
                            if hasattr(mediainfo, 'type'):
                                if hasattr(mediainfo.type, 'value'):
                                    media_type_str = str(mediainfo.type.value)
                                else:
                                    media_type_str = str(mediainfo.type)
                        except Exception as type_err:
                            logger.warning(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–media_typeå¤±è´¥: {type_err}")

                        # è·å–ä»»åŠ¡è¿›åº¦
                        task_id = task.get('id')
                        progress = self._task_progress.get(task_id, 0)

                        # æ„å»ºä»»åŠ¡æ•°æ®
                        task_data = {
                            "task_id": task_id,  # ä½¿ç”¨idè€Œétask_id
                            "title": mediainfo.title or 'æœªçŸ¥æ ‡é¢˜',
                            "media_type": media_type_str,
                            "status": task.get('status', 'pending'),
                            "add_time": add_time_str,
                            "retry_count": task.get('retry_count', 0),
                            "tmdb_id": mediainfo.tmdb_id or 'æ— ',
                            "is_consolidated": task.get('is_consolidated', False),
                            "progress": progress  # æ·»åŠ è¿›åº¦å­—æ®µ
                        }

                        # å¦‚æœæ˜¯æ•´åˆä»»åŠ¡,æ·»åŠ é›†æ•°åˆ—è¡¨
                        if task.get('is_consolidated') and task.get('episodes'):
                            episodes = task.get('episodes', [])
                            task_data["episode_count"] = len(episodes)
                            task_data["episodes"] = episodes
                            # æ„å»ºé›†æ•°æ‘˜è¦
                            episode_summary = f"{len(episodes)}é›†"
                            task_data["episode_info"] = episode_summary
                        else:
                            # å•é›†æˆ–ç”µå½±
                            episode_info = ''
                            if hasattr(mediainfo, 'season') and mediainfo.season:
                                episode_info = f"S{mediainfo.season:02d}"
                                if hasattr(mediainfo, 'episode') and mediainfo.episode:
                                    episode_info += f"E{mediainfo.episode:02d}"
                            elif hasattr(mediainfo, 'episode') and mediainfo.episode:
                                episode_info = f"E{mediainfo.episode:02d}"
                            else:
                                episode_info = '-'
                            task_data["episode_info"] = episode_info
                            task_data["episode_count"] = 0
                            task_data["episodes"] = None

                        result.append(task_data)
                    except Exception as e:
                        logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: å¤„ç†ä»»åŠ¡æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
                        continue

                return {
                    "buffer_count": len(self._buffer_tasks),
                    "consolidate_countdown": consolidate_countdown,
                    "tasks": result
                }
        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–å¾…å¤„ç†ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            return {
                "buffer_count": 0,
                "consolidate_countdown": 0,
                "tasks": []
            }

    def _delete_task(self, payload: dict = None) -> Dict[str, Any]:
        """APIç«¯ç‚¹: åˆ é™¤æŒ‡å®šä»»åŠ¡"""
        if payload is None:
            logger.error("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: åˆ é™¤ä»»åŠ¡è¯·æ±‚æ•°æ®ä¸ºç©º")
            return {"success": False, "message": "è¯·æ±‚æ•°æ®ä¸ºç©º"}

        task_id = payload.get('task_id')
        if not task_id:
            return {"success": False, "message": "æœªæŒ‡å®šä»»åŠ¡ID"}

        with self._lock:
            # ä»å¾…å¤„ç†é˜Ÿåˆ—ä¸­æŸ¥æ‰¾å¹¶åˆ é™¤
            for i, task in enumerate(self._pending_tasks):
                if task.get('id') == task_id:  # ä½¿ç”¨'id'è€Œä¸æ˜¯'task_id'
                    # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                    if task.get('status') == 'processing':
                        return {"success": False, "message": "ä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­,æ— æ³•åˆ é™¤"}

                    # åˆ é™¤ä»»åŠ¡
                    deleted_task = self._pending_tasks.pop(i)

                    # åŒæ—¶ä»processing_tasksä¸­åˆ é™¤(å¦‚æœå­˜åœ¨)
                    if task_id in self._processing_tasks:
                        del self._processing_tasks[task_id]

                    # ä»è¿›åº¦å­—å…¸ä¸­åˆ é™¤
                    if task_id in self._task_progress:
                        del self._task_progress[task_id]

                    # è·å–ä»»åŠ¡æ ‡é¢˜
                    mediainfo = deleted_task.get('mediainfo')
                    title = mediainfo.title if mediainfo else 'æœªçŸ¥'
                    logger.info(f"æ‰‹åŠ¨åˆ é™¤ä»»åŠ¡: {title} (ID: {task_id})")
                    return {"success": True, "message": "ä»»åŠ¡å·²åˆ é™¤"}

            return {"success": False, "message": "æœªæ‰¾åˆ°æŒ‡å®šä»»åŠ¡"}

    def _get_rate_limit_status(self) -> Dict[str, Any]:
        """APIç«¯ç‚¹: è·å–æµæ§çŠ¶æ€"""
        if not self._danmu_server_url or not self._external_api_key:
            return {"error": True, "message": "æœªé…ç½®å¼¹å¹•åº“æœåŠ¡å™¨åœ°å€æˆ–APIå¯†é’¥"}

        try:
            import requests
            # ç¡®ä¿URLä¸ä¼šæœ‰åŒæ–œæ 
            base_url = self._danmu_server_url.rstrip('/')
            url = f"{base_url}/api/control/rate-limit/status"
            params = {"api_key": self._external_api_key}

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()

            # æ£€æŸ¥å“åº”å†…å®¹
            if not response.text:
                logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–æµæ§çŠ¶æ€å¤±è´¥ - æœåŠ¡å™¨è¿”å›ç©ºå“åº”")
                return {"error": True, "message": "æœåŠ¡å™¨è¿”å›ç©ºå“åº”"}

            try:
                data = response.json()
                # âœ… ç›´æ¥è¿”å›data,ä¸åŒ…è£…successå­—æ®µ,è®©MoviePilotæ¡†æ¶è‡ªåŠ¨åŒ…è£…
                return data
            except ValueError as json_err:
                logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–æµæ§çŠ¶æ€å¤±è´¥ - JSONè§£æé”™è¯¯: {str(json_err)}")
                return {"error": True, "message": f"æœåŠ¡å™¨å“åº”æ ¼å¼é”™è¯¯: {str(json_err)}"}

        except requests.exceptions.HTTPError as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–æµæ§çŠ¶æ€å¤±è´¥ - HTTPé”™è¯¯ {e.response.status_code if e.response else 'N/A'}")
            return {"error": True, "message": f"HTTPé”™è¯¯: {str(e)}"}
        except requests.exceptions.RequestException as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–æµæ§çŠ¶æ€å¤±è´¥ - ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")
            return {"error": True, "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: è·å–æµæ§çŠ¶æ€å¤±è´¥ - æœªçŸ¥é”™è¯¯: {str(e)}")
            return {"error": True, "message": f"è·å–æµæ§çŠ¶æ€å¤±è´¥: {str(e)}"}

    def get_form(self) -> Tuple[Optional[List[dict]], Dict[str, Any]]:
        """
        Vueæ¨¡å¼ä¸‹è¿”å›None,ä½†æä¾›åˆå§‹é…ç½®æ•°æ®
        """
        return None, self._get_config()

    def get_api(self) -> List[Dict[str, Any]]:
        """å®šä¹‰APIç«¯ç‚¹ä¾›Vueç»„ä»¶è°ƒç”¨"""
        return [
            {
                "path": "/config",
                "endpoint": self._get_config,
                "methods": ["GET"],
                "auth": "bear",
                "summary": "è·å–å½“å‰é…ç½®"
            },
            {
                "path": "/config",
                "endpoint": self._save_config,
                "methods": ["POST"],
                "auth": "bear",
                "summary": "ä¿å­˜é…ç½®"
            },
            {
                "path": "/queue_stats",
                "endpoint": self._get_queue_stats,
                "methods": ["GET"],
                "auth": "bear",
                "summary": "è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"
            },
            {
                "path": "/pending_tasks",
                "endpoint": self._get_pending_tasks,
                "methods": ["GET"],
                "auth": "bear",
                "summary": "è·å–å¾…å¤„ç†ä»»åŠ¡åˆ—è¡¨"
            },
            {
                "path": "/delete_task",
                "endpoint": self._delete_task,
                "methods": ["POST"],
                "auth": "bear",
                "summary": "åˆ é™¤æŒ‡å®šä»»åŠ¡"
            },
            {
                "path": "/rate_limit_status",
                "endpoint": self._get_rate_limit_status,
                "methods": ["GET"],
                "auth": "bear",
                "summary": "è·å–å¼¹å¹•åº“æµæ§çŠ¶æ€"
            },
            {
                "path": "/consolidate",
                "endpoint": self._trigger_manual_consolidate,
                "methods": ["POST"],
                "auth": "bear",
                "summary": "æ‰‹åŠ¨è§¦å‘æ•´åˆ"
            },
            {
                "path": "/import_task",
                "endpoint": self._trigger_import_task,
                "methods": ["POST"],
                "auth": "bear",
                "summary": "æ‰‹åŠ¨è§¦å‘å•æ¡ä»»åŠ¡å¯¼å…¥"
            }
        ]

    def get_page(self) -> Optional[List[dict]]:
        """Vueæ¨¡å¼ä¸ä½¿ç”¨Vuetifyé¡µé¢å®šä¹‰"""
        return None

    def stop_service(self):
        """åœæ­¢æ’ä»¶æœåŠ¡"""
        logger.info("å¼¹å¹•è‡ªåŠ¨å¯¼å…¥: åœæ­¢æœåŠ¡")
        # æ¸…ç†èµ„æº
        pass

